{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM32e/013u47oW6zumHq2F5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SidCodes0001/RAG-Document-Processing-Bot/blob/main/RAG_Application.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain\n",
        "!pip -q install transformers\n",
        "!pip -q install bitsandbytes accelerate xformers einops\n",
        "!pip -q install datasets loralib sentencepiece\n",
        "!pip -q install pypdf\n",
        "!pip -q install sentence_transformers\n",
        "!pip install chromadb\n",
        "!pip install openai\n",
        "!pip install tiktoken\n"
      ],
      "metadata": {
        "id": "bTTcR88pi-AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import Docx2txtLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "from transformers import HuggingFacePipeline\n",
        "\n",
        "\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b7QQjOfci98r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Documents and Extract Text From Them"
      ],
      "metadata": {
        "id": "UJeMCin4krng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir docs\n"
      ],
      "metadata": {
        "id": "vGxPSJf5i96o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload your files in this docs folder"
      ],
      "metadata": {
        "id": "Bew9lsAMkwyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "document=[]\n",
        "for file in os.listdir(\"docs\"):\n",
        "  if file.endswith(\".pdf\"):\n",
        "    pdf_path=\"./docs/\"+file\n",
        "    loader=PyPDFLoader(pdf_path)\n",
        "    document.extend(loader.load())\n",
        "  elif file.endswith('.docx') or file.endswith('.doc'):\n",
        "    doc_path=\"./docs/\"+file\n",
        "    loader=Docx2txtLoader(doc_path)\n",
        "    document.extend(loader.load())\n",
        "  elif file.endswith('.txt'):\n",
        "    text_path=\"./docs/\"+file\n",
        "    loader=TextLoader(text_path)\n",
        "    document.extend(loader.load())"
      ],
      "metadata": {
        "id": "dFHluMkBi94g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document\n"
      ],
      "metadata": {
        "id": "mg7jxY8wk3XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(document)\n"
      ],
      "metadata": {
        "id": "WktFIQHyk6xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZXGGkMzlk-jZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the Document into Chunks"
      ],
      "metadata": {
        "id": "AP33Ggr_k9tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- character text splitter\n",
        "- RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "ggBHa2xQlBU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_splitter=CharacterTextSplitter(separator='\\n', chunk_size=500, chunk_overlap=100)\n",
        "\n",
        "document_chunks=document_splitter.split_documents(document)\n",
        "\n",
        "document_chunks[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "0qodONZFk9Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Embeddings from Hugging Face, Download the Sentence Transformer Embeddings"
      ],
      "metadata": {
        "id": "r6Opuwa4lTc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n"
      ],
      "metadata": {
        "id": "qt4MqxktlWTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "# embeddings = OpenAIEmbeddings()\n",
        "# embeddings\n"
      ],
      "metadata": {
        "id": "kFx1LqUflka3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Chroma as our Vector Database"
      ],
      "metadata": {
        "id": "zamGMQKjm3y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb=Chroma.from_documents(document_chunks,embedding=embeddings, persist_directory='./data')\n",
        "vectordb.persist()\n",
        "\n"
      ],
      "metadata": {
        "id": "OVBhSbS5m4c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FAISS AS VECTOR DB"
      ],
      "metadata": {
        "id": "HAhuwUlApJdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorstore=FAISS.from_documents(text_chunks, embeddings)\n"
      ],
      "metadata": {
        "id": "gX_5KTBrpKcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "ohNj6xPKnRGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                          use_auth_token=True,)\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                             device_map='auto',\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             use_auth_token=True,\n",
        "                                             load_in_4bit=True,\n",
        "                                            #load_in_8bit=True,\n",
        "                                            # load_in_16bit=True\n",
        "                                             )"
      ],
      "metadata": {
        "id": "SwUbmwoHnWyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Hugging Face Pipeline"
      ],
      "metadata": {
        "id": "4I2g6rtnnoDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pipe=pipeline(\"text-generation\",\n",
        "              model=model,\n",
        "              tokenizer=tokenizer,\n",
        "              torch_dtype=torch.bfloat16,\n",
        "              device_map='auto',\n",
        "              max_new_tokens=512,\n",
        "              min_new_tokens=-1,\n",
        "              top_k=30,\n",
        "              eos_token_id=tokenizer.eos_token_id\n",
        "              )"
      ],
      "metadata": {
        "id": "qptniWEwnopA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n"
      ],
      "metadata": {
        "id": "GC2u_A80nz_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for open AI llm"
      ],
      "metadata": {
        "id": "a34nFWrFn1Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# llm=ChatOpenAI(temperature=0.7, model_name='gpt-3.5-turbo')\n"
      ],
      "metadata": {
        "id": "gDQ1G2kfn328"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a memory object which is necessary to track inputs/outputs and hold a conversation"
      ],
      "metadata": {
        "id": "WjplCud7oGDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
      ],
      "metadata": {
        "id": "Ap8fjPqooGsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Conversation Retrieval QA Chain\n",
        "\n",
        "- The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.\n",
        "\n"
      ],
      "metadata": {
        "id": "DZJ-PVRUoMm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_qa=ConversationalRetrievalChain.from_llm(llm=llm,\n",
        "                                             retriever=vectordb.as_retriever(search_kwargs={'k':6}),\n",
        "                                             verbose=False, memory=memory)"
      ],
      "metadata": {
        "id": "1MFTUOrdoNSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RETRIEVAL QA SOURCE CHAIN"
      ],
      "metadata": {
        "id": "zos4XxkVptEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())\n",
        "# result=chain({\"question\": \"How good is Vicuna?\"}, return_only_outputs=True)\n"
      ],
      "metadata": {
        "id": "MSQatLV4pbVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=pdf_qa({\"question\":\"YOLOv7 is trained on which dataset\"})\n",
        "result['answer']\n"
      ],
      "metadata": {
        "id": "s1TxoaDUoe4e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}